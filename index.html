<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>SurgVLM</title>
  <meta name="description" content="SurgVLM: Large Vision-Language Model for Surgical Intelligence">

  <!-- Open Graph -->


  <!-- Bulma & MDB -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.24/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">

  <!-- Fonts & Icons -->
  <link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"
      integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous">
  <link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css"
      crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
      href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Lora|Google+Sans|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/.css" /> -->

  <!-- Styles -->
  <link rel="shortcut icon" href="assets/img/favicon.ico">
  <link rel="stylesheet" href="assets/css/main.css">

  <link rel="canonical" href="/">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css"/>


  <!-- Theming-->
  <style>
    body.light-theme {
        background-color: #ffffff;
        color: #000000;
    }

    body.dark-theme {
        background-color: #121212;
        color: #ffffff;
    }

    .theme-switch {
        position: fixed;
        top: 10px;
        right: 10px;
    }
  </style>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XD4J271NBT"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag () { dataLayer.push(arguments); }
      gtag('js', new Date());

      gtag('config', 'G-XD4J271NBT');
  </script>


  <!-- leaderboard -->
  <!-- <script type="text/javascript" src="static/js/sort-table.js" defer></script> -->

  <!-- Data Sample -->
  <!-- <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script> -->

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script> -->
  <style>
    .center-container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
    }
  </style>
  <style>
    .highlights-list {
        list-style-type: none;
        padding: 0;
    }

    .highlights-list > li {
        margin-top: 1.5em;
    }

    .highlights-list strong {
        color: #007BFF;
    }

    .cta {
        margin-top: 1.5em;
        font-size: 1.1em;
    }

    .link {
        color: #FF5733;
        text-decoration: none;
        transition: color 0.3s ease;
    }

    .link:hover {
        color: #C70039;
    }
  </style>
  <style>
    .qa-section {
        margin-top: 10px;
        text-align: center;
    }
    
    .question {
        font-weight: bold;
        font-size: 1.1em;
        color: #333;
    }
    
    .answer {
        font-size: 1em;
        color: #555;
        margin-top: 5px;
        font-style: italic;
    }
    </style>
    <style>
        .box {
            display: none; /* Hide all boxes initially */
        }
    </style>

      
  <!-- MathJax -->
  <script type="text/javascript">
      window.MathJax = {
          tex: {
              tags: 'ams'
          }
      };
  </script>
  <script defer type="text/javascript" id="MathJax-script"
      src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>

</head>

<body>
    <!-- Header -->

      
  <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
          <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
              <span aria-hidden="true"></span>
              <span aria-hidden="true"></span>
              <span aria-hidden="true"></span>
          </a>
      </div>
      <!-- <div class="navbar-menu">
          <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
              <a class="navbar-item" href="https://omnimmi.github.io">
                  <span class="icon">
                      <i class="fas fa-home"></i>
                  </span>
              </a>

              <div class="navbar-item has-dropdown is-hoverable">
                  <a class="navbar-link">
                      More Research
                  </a>
                  <div class="navbar-dropdown">

                      <a class="navbar-item" href="https://videollamb.github.io/">
                          VideoLLaMB
                      </a>  
                    
                      <a class="navbar-item" href="https://bigai-nlco.github.io/ExoViP/">
                          ExoViP
                      </a>
                      
                      <a class="navbar-item" href="https://videohallucer.github.io/">
                          VideoHallucer
                      </a>
                      
                      <a class="navbar-item" href="https://github.com/bigai-nlco/VideoTGB">
                          VideoTGB
                      </a>
                      
                      <a class="navbar-item" href="https://vstar-benchmark.github.io/">
                          VSTAR
                      </a>
                      
                  </div>
              </div>
          </div>

      </div> -->
  </nav>


      <!-- Content -->

      <div class="container is-fullhd mt-5">
          <section class="hero">
              <div class="hero-body">
                  <div class="container is-max-desktop">
                      <div class="columns is-centered">
                          <div class="column has-text-centered">
                              <div style="display: flex;padding-bottom: 20px;">
                                  <!-- <img src="/assets/img/logo.png"
                                      width="150px" style="vertical-align: middle;"> -->
                                  <h1 class="title is-2 publication-title"><span style="display: block;">SurgVLM: Large Vision-Language Model for Surgical Intelligence</span></h1>
                              </div>
                              
                              <div class="is-size-4 publication-venue">
                                  
                                <!-- <span class="publication-venue">CVPR 2025</span> -->
                                
                            </div>

                            <!-- <br> -->

                              <!-- <div class="is-size-5 publication-authors">
                                  
                                  <span class="author-block"><a href="https://fxlyz.github.io/">Zhitao Zeng</a><sup>1,3,4</sup></span>,
                                  
                                  <span class="author-block"><a>Zhuo Zhu</a><sup>1</sup></span>, 

                                  <span class="author-block"><a href="https://jiaxiaojunqaq.github.io/">Xiaojun Jia</a><sup>2</sup></span>,

                                  <span class="author-block"><a href="https://patrick-tssn.github.io/">Yuxuan Wang</a><sup>3,4</sup></span>,

                                  <span class="author-block"><a>Erli Zhang</a><sup>1</sup></span>,

                                  <span class="author-block"><a>Jiaan Zhang</a><sup>1</sup></span>,

                                  <span class="author-block"><a>Changhan Low</a><sup>1</sup></span>,

                                  <span class="author-block"><a>Jian Jiang</a><sup>6</sup></span>,

                                  <span class="author-block"><a href="https://scholar.google.com/citations?user=FZSKG-AAAAAJ&hl=en">Junde Wu</a><sup>7</sup></span>,

                                  <span class="author-block"><a href="https://people.csail.mit.edu/yban/index.html">Yutong Ban</a><sup>6</sup></span>,

                                  <span class="author-block"><a href="https://zilongzheng.github.io">Zilong Zheng</a><sup>3,4</sup></span>,

                                  <span class="author-block"><a href="https://scholar.google.com/citations?user=PDgp6OkAAAAJ&hl=en">Xiaochun Cao</a><sup>8</sup></span>,
                                  
                                  <span class="author-block"><a href="https://scholar.google.com.hk/citations?user=iHh7IJQAAAAJ&hl=zh-CN">Qi Dou</a><sup>5</sup></span>,

                                  <span class="author-block"><a href="https://personal.ntu.edu.sg/yangliu/">Yang Liu</a><sup>2</sup></span>,
                                  
                                  <span class="author-block"><a href="https://yuemingjin.github.io">Yueming Jin</a><sup>1, <i class="fa fa-envelope"></i></sup></span>
                                  
                              </div> -->
                              

                              
                              <div class="is-size-5 publication-authors">
                                  
                                  <span class="author-block"><a href="https://fxlyz.github.io/">Zhitao Zeng</a><sup>1,†</sup></span>,
                                  
                                  <span class="author-block"><a>Zhu Zhuo</a><sup>1,†</sup></span>, 

                                  <span class="author-block"><a href="https://jiaxiaojunqaq.github.io/">Xiaojun Jia</a><sup>2,†</sup></span>,

                                  <span class="author-block"><a>Erli Zhang</a><sup>1,†</sup></span>,

                                  <span class="author-block"><a href="https://scholar.google.com/citations?user=FZSKG-AAAAAJ&hl=en">Junde Wu</a><sup>7</sup></span>,

                                  <span class="author-block"><a>Jiaan Zhang</a><sup>1</sup></span>,

                                  <span class="author-block"><a href="https://patrick-tssn.github.io/">Yuxuan Wang</a><sup>6</sup></span>,

                                  <span class="author-block"><a>Chang Han Low</a><sup>1</sup></span>,

                                  <span class="author-block"><a>Jian Jiang</a><sup>5</sup></span>,

                                  <span class="author-block"><a href="https://zilongzheng.github.io">Zilong Zheng</a><sup>6</sup></span>,

                                  <span class="author-block"><a href="https://scholar.google.com/citations?user=PDgp6OkAAAAJ&hl=en">Xiaochun Cao</a><sup>4</sup></span>,
                                  
                                  <span class="author-block"><a href="https://people.csail.mit.edu/yban/index.html">Yutong Ban</a><sup>5</sup></span>,

                                  <span class="author-block"><a href="https://scholar.google.com.hk/citations?user=iHh7IJQAAAAJ&hl=zh-CN">Qi Dou</a><sup>3</sup></span>,

                                  <span class="author-block"><a href="https://personal.ntu.edu.sg/yangliu/">Yang Liu</a><sup>2</sup></span>,
                                  
                                  <span class="author-block"><a href="https://yuemingjin.github.io">Yueming Jin</a><sup>1,*</sup></span>
                                  
                              </div>
                              

                              
                              <div class="is-size-5 publication-authors">
                                  
                                  <span class="author-block"><sup>1</sup>National University of Singapore</span>, 
                                  
                                  <span class="author-block"><sup>2</sup>Nanyang Technological University</span>, 
                                  
                                  <span class="author-block"><sup>3</sup>Chinese University of Hong Kong</span>,
                                  
                                  <span class="author-block"><sup>4</sup>Sun Yat-sen University</span>,
                                  
                                  <span class="author-block"><sup>5</sup>Shanghai Jiao Tong University</span>,
                                  
                                  <span class="author-block"><sup>6</sup>State Key Laboratory of General Artificial Intelligence, BIGAI</span>,
                                  
                                  <span class="author-block"><sup>7</sup>University of Oxford</span>
                              </div>
                              

                              

                              <div class="column has-text-centered">
                                  <div class="publication-links">
                                      
                                      
                                      <span class="link-block">
                                          <a href=""
                                              class="external-link button is-normal is-rounded is-dark">
                                              <span class="icon">
                                                  <i class="ai ai-arxiv"></i>
                                              </span>
                                              <span>arXiv</span>
                                          </a>
                                      </span>
                                      
                                      
                                      
                                      <!-- Code Link. -->
                                      <span class="link-block">
                                          <a href=""
                                              class="external-link button is-normal is-rounded is-dark">
                                              <span class="icon">
                                                  <i class="fab fa-github"></i>
                                              </span>
                                              <span>Code (SurgVLM)</span>
                                          </a>
                                      </span>
                                      
                                    <br>  

                                      <span class="link-block">
                                        <a href=""
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                &#129303;
                                            </span>
                                            <span>SurgVLM-DB</span>
                                        </a>
                                    </span>
                                      

                                    <span class="link-block">
                                      <a href=""
                                          class="external-link button is-normal is-rounded is-dark">
                                          <span class="icon">
                                              &#129303;
                                          </span>
                                          <span>SurgVLM-MMBench</span>
                                      </a>
                                  </span>
                                      
                                      <span class="link-block">
                                          <a href=""
                                              class="external-link button is-normal is-rounded is-dark">
                                              <span class="icon">
                                                  &#129303;
                                              </span>
                                              <span>SurgVLM-7B</span>
                                          </a>
                                      </span>

                                      <span class="link-block">
                                        <a href=""
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                &#129303;
                                            </span>
                                            <span>SurgVLM-32B</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                      <a href=""
                                          class="external-link button is-normal is-rounded is-dark">
                                          <span class="icon">
                                              &#129303;
                                          </span>
                                          <span>SurgVLM-72B</span>
                                      </a>
                                  </span>
                                      
                                      

                                      <!-- <span class="link-block">
                                        <a href="https://huggingface.co/ColorfulAI/LLaVA-NeXT-Audio"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                &#129303;
                                            </span>
                                            <span>LLaVA-NeXT-Audio</span>
                                        </a>
                                    </span> -->
                                      
                                      <!-- <span class="link-block">
                                          <a href=" "
                                              class="external-link button is-normal is-rounded is-dark">
                                              <span class="icon">
                                                  <i class="fas fa-globe"></i>
                                              </span>
                                              <span>Demo (Coming Soon)</span>
                                          </a>
                                      </span> -->
                                      
                                      
                                  </div>
                              </div>
                          </div>
                      </div>
                  </div>
              </div>
          </section>


          <section class="hero teaser">
            <div class="container is-max-desktop">
              <div class="hero-body">
          <div class="columns is-centered has-text-centered">
          
            <div class="column is-four-fifths">
              <figure class="image">
                    <img src="assets/img/new_Figure1.png" />
                    <!-- <figcaption><span class="dnerf">Figure 1.</span> <b>An overview of OmniMMI.</b> </figcaption> -->
              </figure>
              </div>
          </div>
          
          <figcaption style="padding-top:10px; text-align: left;"><span class="dnerf">Figure 1.</span>
            <b>Illustration of our Surgical Multimodal database SurgVLM-DB</b>. <b>a</b> SurgVLM-DB contains 16 surgical types and 18 anatomical structures, reflecting the broad applicability of SurgVLM.
            <b>b</b> SurgVLM-DB contain 1.181M annotated images with 7.79M conversations, and the distribution demonstrates large-scale comprehensive data in diverse surgical types, ensuring the robustness of SurgVLM.</figcaption>
          
              </div>
            </div>
          </section>

          
          

  <!-- <section class="section">
      <div class="container is-max-desktop">

      <h2 class="title has-text-centered" id="abstract">Abstract</h2>

      <p>OmniMMI is a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 real-world interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtask. Moreover, we propose a novel framework, Multimodal Multiplexing Modeling (M4), designed to enhance real-time interactive reasoning with minimum finetuning on pre-trained MLLMs</p>

      <p>✨ Highlights:</p>

      <ol>
        <li>
          <p><strong>A Comprehensive Multi-modal Interaction Benchmark OmniMMI</strong></p>
          <ul>
            <li>
              <p><strong>1.1 Streaming Temporal State Awareness.</strong> Streaming video understanding must build an understanding w.r.t. the current and historical temporal state incrementally, without accessing the future context. This contrasts with traditional MLLMs that can leverage the entire multi-modal contexts, posing challenges in our distinguished tasks of action prediction (AP), state grounding (SG), and multi-turn dependencies (MD).</p>
            </li>
            <li>
              <p><strong>1.2 Proactive Reasoning and Turn-Taking.</strong> Generating responses proactively and appropriately anticipating the turn-taking time spot w.r.t. user's intentions and dynamic contexts is a crucial feature for general interactive agents. This typically requires models to identify speakers (SI), distinguish between noise or legitimate query (PT), and proactively initiate a response (PA).</p>
            </li>
          </ul>
        </li>
        <li>
          <p><strong>Multi-modal Multiplexing Modeling</strong></p>
          <ul>
            <li>
              <p><strong>2.1 A small video-free synthetic instruction finetuning dataset M4-IT.</strong> We propose M4-IT, comprising four components: (i) the original instruction, which is a data replay from the instruction data of our base model, in our work, we use the LLaVA-NeXT; (ii) interleaved image-text instruction, which is created by reordering the question and image components of the original instruction; (iii) noise instruction, where GPT-4 is prompted to automatically generate statements that do not require a response; and (iv) stop instruction, where GPT-4 is prompted to generate stop phrases for the stop instruction.</p>
            </li>
            <li>
              <p><strong>2.2 A multimodal multiplexing MLLM M4.</strong> The M4 introduces improvements in several key areas: Firstly, it enhances proactive generation by autonomously producing subsequent responses during video streaming without the need for human input. Secondly, it improves proactive interruption by assessing the legitimacy of new queries against mere noise in a single step. Lastly, it boasts efficient parallel decoding, allowing the model to decode the next token concurrently with processing the inputs.</p>
            </li>
          </ul>
        </li>
      </ol>
      
    </div>




  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
        <h2 class="title has-text-centered" id="abstract">Abstract</h2>
        <p>
          Foundation models have achieved transformative success across biomedical domains by enabling holistic understanding of multimodal data. However, their application in surgery remains underexplored. Surgical intelligence presents unique challenges - requiring surgical visual perception, temporal analysis, and reasoning. Existing general-purpose vision-language models fail to address these needs due to insufficient domain-specific supervision and the lack of a large-scale high-quality surgical database. To bridge this gap, we propose SurgVLM, one of the first large vision-language foundation models for surgical intelligence, where this single universal model can tackle versatile surgical tasks.
          To enable this, we construct a large-scale multimodal surgical database, SurgVLM-DB, comprising over 1.81 million frames with 7.79 million conversations, spanning more than 16 surgical types and 18 anatomical structures. 
          We unify and reorganize 23 public datasets across 10 surgical tasks, followed by standardizing labels and doing hierarchical instruction alignment to facilitate comprehensive coverage of gradually finer-grained surgical tasks, from visual perception, temporal analysis, to high-level reasoning.
          Building upon this comprehensive dataset, we propose SurgVLM, which is built upon Qwen2.5, and undergoes multimodal projector pretraining to align visual-text representations, followed by instruction tuning to specific surgical tasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for method evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets in surgical domain, covering several crucial downstream tasks. Based on SurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants: SurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive comparisons with 16 mainstream general-purpose VLMs (e.g., GPT-4o, Gemini 2.0 Flash, Qwen2.5-Max).
          Extensive experimental results show that the proposed SurgVLM consistently surpasses state-of-the-art general-purpose VLMs.
          SurgVLM-72B achieves 75.4\% improvement on overall arena score compared with Gemini 2.0 Flash, including a 96.5\% improvement in phase recognition, 87.7\% in action recognition, 608.1\% in triplet prediction, 198.5\% in instrument localization, 28.9\% in critical view safety detection, and 59.4\% in a comprehensive multi-task VQA dataset.
          Beyond raw performance gains, SurgVLM demonstrates robust generalization and open‑vocabulary QA, establishing a scalable, accurate, and clinically reliable paradigm for unified surgical intelligence. Model weights, dataset, benchmark, and code will be available at \url{https://jinlab-imvr.github.io/SurgVLM}.
        </p>
        <p>✨ <strong>Highlights:</strong></p>
        <ol class="highlights-list">
            <li>
                <p><strong>SurgVLM-DB: A Large-scale Surgical Multimodal Database</strong></p>
                <!-- <ul>
                    <li>
                        <p><strong>Streaming Temporal State Awareness:</strong> This requires understanding current and historical video states without future context, challenging tasks like action prediction, state grounding, and multi-turn dependencies.</p>
                    </li>
                    <li>
                        <p><strong>Proactive Reasoning and Turn-Taking:</strong> Models must generate responses proactively, identifying speakers, distinguishing noise from queries, and initiating responses appropriately.</p>
                    </li>
                </ul> -->
            </li>
            
            <li>
              <p><strong>SurgVLM-Bench: A Comprehensive Surgical Benchmarks for VLMs</strong></p>
              <!-- <ul>
                  <li>
                      <p><strong>Streaming Temporal State Awareness:</strong> This requires understanding current and historical video states without future context, challenging tasks like action prediction, state grounding, and multi-turn dependencies.</p>
                  </li>
                  <li>
                      <p><strong>Proactive Reasoning and Turn-Taking:</strong> Models must generate responses proactively, identifying speakers, distinguishing noise from queries, and initiating responses appropriately.</p>
                  </li>
              </ul> -->
          </li>

            <li>
                <p><strong>SurgVLM: Large Vision-Language Model for Surgical Intelligence</strong></p>
                <!-- <ul>
                    <li>
                        <p><strong>M4-IT Dataset:</strong> A synthetic instruction finetuning dataset with components interleaved image-text instruction, noise instruction, and stop instruction.</p>
                    </li>
                    <li>
                        <p><strong>M4 Model:</strong> Enhances proactive response generation, assesses new queries against noise, by enabling parallel decoding.</p>
                    </li>
                </ul> -->
            </li>
        </ol>
        <!-- <p class="cta">
            If you're interested in the M4 model, check the introduction 
            <a href="https://omnimmi.github.io/m4.html" class="link">here</a>.
        </p> -->
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
  <div class="columns is-centered has-text-centered">
  
    <div class="column is-four-fifths">
      <figure class="image">
            <img src="assets/img/Figure2.png" />
      </figure>
      <figcaption style="padding-top:10px; text-align: left;"><span class="dnerf">Figure 2.</span>
    <b>a</b>, The data pipeline of SurgVLM-DB can be divided into three stages including data cleaning \& refinement, contextual enhancement, and augmentation \& diversification.
    <b>b</b>, Overview of task hierarchy in SurgVLM-DB containing 10 surgical tasks, ranging from visual perception to scene understanding and decision-making. Tasks are organized based on increasing contextual dependency, task complexity, and clinical impact.
    <b>c</b>, The comparison shows SurgVLM-DB is an advanced surgical multimodal dataset with largest number of image, annotation, and surgical procedures indicated by the size of bubble.
    <b>d</b>, 7.79 million samples distribution of SurgVLM-DB according to 3 intelligence-level and 10 surgical tasks.</figcaption>
      </div>
  </div>
  
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
  <div class="columns is-centered has-text-centered">
  
    <div class="column is-four-fifths">
      <figure class="image">
            <img src="assets/img/Figure3.png" />
      </figure>
      <figcaption style="padding-top:10px; text-align: left;"><span class="dnerf">Figure 3.</span> 
    <b>a</b>, Leaderboard of VLMs on SurgVLM-Bench, demostrating superior performance of SurgVLM compared with general-purpose VLMs.
    <b>b</b>, Comprehensive comparison with Gemini 2.0 Flash, Qwen2.5 Max, and GPT-4o. SurgVLM consistently outperforms commercial VLMs across 24 metrics.
    <b>c</b>, Detailed comparison with 16 general-purpose VLMs on the most important metrics across six surgical tasks. All SurgVLM models achieve state-of-the-art performance.</figcaption>
      </div>
  </div>
  
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#efeff081">
    <div class="container">
      <h2 class="title is-3 has-text-centered" id="surgical-arena-results">Leaderboard of Vision-Language Models on SurgVLM-Bench</h2>
      <h3 class="title is-6 has-text-centered" id="results-contact">Submit Your Results: zhitao@nus.edu.sg</h3>
      <div class="table-container has-text-centered" style="font-size:15px">
        <p style="text-align: center; margin-bottom: 20px;">
          The leaderboard is ranked by the <strong>Arena Score</strong> obtained by summing the most important metrics across six surgical tasks.
          Higher scores indicate better performance.
        </p>
        <table class="table is-fullwidth is-narrow is-hoverable" id="table2">
          <thead>
            <tr style="background-color: #d3d3d3;">
              <th style="text-align: center;"><strong>Rank</strong></th>
              <th style="text-align: left;"><strong>Model</strong></th>
              <th style="text-align: center;"><strong>Evaluation</strong></th>
              <th style="text-align: center;"><strong>Arena Score ↑</strong></th>
              <th style="text-align: center;"><strong>Phase Acc</strong></th>
              <th style="text-align: center;"><strong>Action Acc</strong></th>
              <th style="text-align: center;"><strong>Triplet Acc</strong></th>
              <th style="text-align: center;"><strong>CVS Acc</strong></th>
              <th style="text-align: center;"><strong>VQA Acc</strong></th>
              <th style="text-align: center;"><strong>Loc mIoU</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr style="background-color: #b7e4c7;"><td>1</td><td style="text-align:left;">SurgVLM-72B Lora-tuning (MCQ)</td><td>MCQ</td><td>336.21</td><td>69.66</td><td>43.1</td><td>12.52</td><td>76.73</td><td>75.2</td><td>59.0</td></tr>
            <tr style="background-color: #d0edd8;"><td>2</td><td style="text-align:left;">SurgVLM-72B Lora-tuning (Openset)</td><td>Openset</td><td>331.86</td><td>76.40</td><td>42.9</td><td>13.10</td><td>76.60</td><td>63.46</td><td>59.4</td></tr>
            <tr><td>3</td><td style="text-align:left;">SurgVLM-32B Freeze-tuning (Openset)</td><td>Openset</td><td>306.91</td><td>71.20</td><td>40.1</td><td>12.98</td><td>74.51</td><td>59.72</td><td>48.4</td></tr>
            <tr><td>4</td><td style="text-align:left;">SurgVLM-7B Full-tuning (Openset)</td><td>Openset</td><td>290.78</td><td>70.30</td><td>45.8</td><td>4.15</td><td>76.86</td><td>59.67</td><td>34.0</td></tr>
            <tr><td>5</td><td style="text-align:left;">Gemini 2.0 Flash</td><td>MCQ</td><td>191.70</td><td>38.89</td><td>24.4</td><td>1.85</td><td>59.61</td><td>47.05</td><td>19.9</td></tr>
            <tr><td>6</td><td style="text-align:left;">Qwen2.5-VL-72B-Instruct</td><td>MCQ</td><td>184.85</td><td>29.30</td><td>28.2</td><td>1.27</td><td>41.69</td><td>42.19</td><td>42.2</td></tr>
            <tr><td>7</td><td style="text-align:left;">Qwen2.5-VL-32B-Instruct</td><td>MCQ</td><td>184.40</td><td>37.23</td><td>31.8</td><td>0.98</td><td>60.53</td><td>42.46</td><td>11.4</td></tr>
            <tr><td>8</td><td style="text-align:left;">Qwen2.5-VL-7B-Instruct</td><td>MCQ</td><td>175.20</td><td>30.45</td><td>31.1</td><td>0.35</td><td>65.88</td><td>36.82</td><td>10.6</td></tr>
            <tr><td>9</td><td style="text-align:left;">Qwen 2.5 Max</td><td>MCQ</td><td>174.37</td><td>34.79</td><td>28.3</td><td>0.35</td><td>34.77</td><td>36.16</td><td>40.0</td></tr>
            <tr><td>10</td><td style="text-align:left;">InternVL3-78B</td><td>MCQ</td><td>172.97</td><td>27.32</td><td>29.5</td><td>0.52</td><td>50.20</td><td>36.33</td><td>29.1</td></tr>
            <tr><td>11</td><td style="text-align:left;">Llama-4-Scout-17B-16E-Instruct</td><td>MCQ</td><td>163.84</td><td>35.77</td><td>25.1</td><td>0.58</td><td>37.39</td><td>37.00</td><td>28.0</td></tr>
            <tr><td>12</td><td style="text-align:left;">Mistral-Small-3.1-24B-Instruct-2503</td><td>MCQ</td><td>156.98</td><td>22.61</td><td>12.5</td><td>0.46</td><td>68.10</td><td>36.41</td><td>16.9</td></tr>
            <tr><td>13</td><td style="text-align:left;">InternVL3-8B</td><td>MCQ</td><td>146.42</td><td>23.88</td><td>29.3</td><td>2.08</td><td>48.24</td><td>34.72</td><td>8.2</td></tr>
            <tr><td>14</td><td style="text-align:left;">MiniCPM-O-2_6</td><td>MCQ</td><td>140.34</td><td>17.75</td><td>30.8</td><td>0.06</td><td>35.95</td><td>35.48</td><td>20.3</td></tr>
            <tr><td>15</td><td style="text-align:left;">Gemma3-27B-it</td><td>MCQ</td><td>138.93</td><td>14.08</td><td>33.2</td><td>0.06</td><td>38.04</td><td>35.95</td><td>17.6</td></tr>
            <tr><td>16</td><td style="text-align:left;">Phi-4-Multimodal-Instruct</td><td>MCQ</td><td>131.10</td><td>22.45</td><td>15.1</td><td>0.12</td><td>58.43</td><td>34.20</td><td>0.8</td></tr>
            <tr><td>17</td><td style="text-align:left;">MiniCPM-V-2_6</td><td>MCQ</td><td>128.77</td><td>15.20</td><td>24.3</td><td>0</td><td>38.69</td><td>33.28</td><td>17.3</td></tr>
            <tr><td>18</td><td style="text-align:left;">GPT-4o</td><td>MCQ</td><td>118.71</td><td>36.43</td><td>28.1</td><td>1.50</td><td>6.67</td><td>38.31</td><td>7.7</td></tr>
            <tr><td>19</td><td style="text-align:left;">LLava-1.5-7B</td><td>MCQ</td><td>112.57</td><td>23.46</td><td>5.1</td><td>0</td><td>25.49</td><td>31.42</td><td>27.1</td></tr>
            <tr><td>20</td><td style="text-align:left;">Skywork-R1V-38B</td><td>MCQ</td><td>107.64</td><td>6.37</td><td>12.3</td><td>0</td><td>43.79</td><td>34.58</td><td>10.6</td></tr>
          </tbody>
        </table>
        <p><span class="dnerf">Table 1.</span> <b>Quantitive comparison of VLMs on SurgVLM-Bench across six surgical tasks.</b></p>
      </div>
    </div>
  </section>
  

  <!-- <section class="section" style="background-color:#efeff081">
    <div class="container">
      <h2 class="title is-3 has-text-centered" id="surgical-arena-results">Surgical Multimodal Arena Leaderboard</h2>
      <h3 class="title is-6 has-text-centered" id="results-contact">Submit Your Results: zhitao@nus.edu.sg</h3>
      <div class="table-container has-text-centered" style="font-size:15px">
        <p style="text-align: center; margin-bottom: 20px;">
          The leaderboard is ranked by the <strong>Arena Score</strong> obtained by summing the most important metrics across six surgical tasks.
          Higher scores indicate better performance.
        </p>
        <table class="table is-fullwidth is-narrow is-hoverable" id="table2">
          <thead>
            <tr style="background-color: #d3d3d3;">
              <th style="text-align: center;"><strong>Rank</strong></th>
              <th style="text-align: left;"><strong>Model</strong></th>
              <th style="text-align: center;"><strong>LLM</strong></th>
              <th style="text-align: center;"><strong>Vision Encoder</strong></th>
              <th style="text-align: center;"><strong>Evaluation</strong></th>
              <th style="text-align: center;"><strong>Arena Score ↑</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr style="background-color: #b7e4c7;">
              <td>1</td>
              <td style="text-align: left;">SurgVLM-72B Lora-tuning (Ours)</td>
              <td>Qwen2.5-72B</td>
              <td>DFN CLIP</td>
              <td>MCQ</td>
              <td><strong>336.21</strong></td>
            </tr>
            <tr style="background-color: #d0edd8;">
              <td>2</td>
              <td style="text-align: left;">SurgVLM-72B Lora-tuning (Ours)</td>
              <td>Qwen2.5-72B</td>
              <td>DFN CLIP</td>
              <td>Openset</td>
              <td><strong>331.86</strong></td>
            </tr>
            <tr style="background-color: #e2f6e5;">
              <td>3</td>
              <td style="text-align: left;">SurgVLM-32B Freeze-tuning (Ours)</td>
              <td>Qwen2.5-32B</td>
              <td>DFN CLIP</td>
              <td>Openset</td>
              <td><strong>306.91</strong></td>
            </tr>
            <tr style="background-color: #f0fbf0;">
              <td>4</td>
              <td style="text-align: left;">SurgVLM-7B Full-tuning (Ours)</td>
              <td>Qwen2.5-7B</td>
              <td>SIGLIP2</td>
              <td>Openset</td>
              <td><strong>290.78</strong></td>
            </tr>
            <tr style="background-color: #ffecec;">
              <td>5</td>
              <td style="text-align: left;">Gemini 2.0 Flash</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>191.70</td>
            </tr>
            <tr>
              <td>6</td>
              <td style="text-align: left;">Qwen2.5-VL-72B-Instruct</td>
              <td>Qwen2.5-72B</td>
              <td>DFN CLIP</td>
              <td>MCQ</td>
              <td>184.85</td>
            </tr>
            <tr>
              <td>7</td>
              <td style="text-align: left;">Qwen2.5-VL-32B-Instruct</td>
              <td>Qwen2.5-32B</td>
              <td>DFN CLIP</td>
              <td>MCQ</td>
              <td>184.40</td>
            </tr>
            <tr>
              <td>8</td>
              <td style="text-align: left;">Qwen2.5-VL-7B-Instruct</td>
              <td>Qwen2.5-7B</td>
              <td>DFN CLIP</td>
              <td>MCQ</td>
              <td>175.20</td>
            </tr>
            <tr>
              <td>9</td>
              <td style="text-align: left;">Qwen 2.5 Max</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>174.37</td>
            </tr>
            <tr>
              <td>10</td>
              <td style="text-align: left;">InternVL3-78B</td>
              <td>InternViT-6B-448px-V2_5</td>
              <td>Qwen2.5-72B</td>
              <td>MCQ</td>
              <td>172.97</td>
            </tr>
            <tr>
              <td>11</td>
              <td style="text-align: left;">Llama-4-Scout-17B-16E-Instruct</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>163.84</td>
            </tr>
            <tr>
              <td>12</td>
              <td style="text-align: left;">Mistral-Small-3.1-24B-Instruct-2503</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>156.98</td>
            </tr>
            <tr>
              <td>13</td>
              <td style="text-align: left;">InternVL3-8B</td>
              <td>InternViT-300M-448px-V2_5</td>
              <td>Qwen2.5-7B</td>
              <td>MCQ</td>
              <td>146.42</td>
            </tr>
            <tr>
              <td>14</td>
              <td style="text-align: left;">MiniCPM-O-2_6</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>140.34</td>
            </tr>
            <tr>
              <td>15</td>
              <td style="text-align: left;">Gemma3-27B-it</td>
              <td>-</td>
              <td>SIGLIP</td>
              <td>MCQ</td>
              <td>138.93</td>
            </tr>
            <tr>
              <td>16</td>
              <td style="text-align: left;">Phi-4-Multimodal-Instruct</td>
              <td>Phi-4-Mini</td>
              <td>SIGLIP</td>
              <td>MCQ</td>
              <td>131.10</td>
            </tr>
            <tr>
              <td>17</td>
              <td style="text-align: left;">MiniCPM-V-2_6</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>128.77</td>
            </tr>
            <tr>
              <td>18</td>
              <td style="text-align: left;">GPT-4o-2024-0806</td>
              <td>-</td>
              <td>-</td>
              <td>MCQ</td>
              <td>118.71</td>
            </tr>
            <tr>
              <td>19</td>
              <td style="text-align: left;">LLava-1.5-7B</td>
              <td>LLama2</td>
              <td>CLIP</td>
              <td>MCQ</td>
              <td>112.57</td>
            </tr>
            <tr>
              <td>20</td>
              <td style="text-align: left;">Skywork-R1V-38B</td>
              <td>DeepSeek-R1-Distill-Qwen-32B</td>
              <td>-</td>
              <td>MCQ</td>
              <td>107.64</td>
            </tr>
          </tbody>
        </table>
        <p><span class="dnerf">Table 1.</span> <b>Quantitive comparison of VLMs on Surgical Multimodal Benchmark across six surgical tasks.</b></p>
      </div>
    </div>
  </section> -->


  <!-- <section class="section">
    <div class="container">
        <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
                <h2 class="title is-3">Data Samples</h2>
                <p style="text-align: center; margin-bottom: 20px;">
                  We provide a selection of short videos to reduce latency. If you're interested in exploring more, please refer to our complete dataset.
                </p>
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <video width="50%" controls>
                                <source src="assets/vid/ap.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p><strong>Task: Action Prediction (AP)</strong></p>
                            <p class="question">I have the duty to Prepare the cooking pot and bowls. The video showcases the task's progress, what do you suggest as my next move?</p>
                            <p class="answer">pour water in cooking pot</p>
                        </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                        <video width="50%" controls>
                            <source src="assets/vid/sg.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p><strong>Task: Dyanmic State Grounding (SG)</strong></p>
                        <p class="question">What is the current step in the rice cooking process?</p>
                        <p class="answer">1s-2s: Taking out rice</p>
                        <p class="answer">3s-4s: Soaking and washing rice</p>
                        <p class="answer">5s-7s: Putting rice into rice cooker</p>
                      </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                        <video width="50%" controls>
                            <source src="assets/vid/md.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p><strong>Task: Multi-turn Dependency Reasoning (MD)</strong></p>
                        <p class="question">0s-1s: What is shown in the beginning of the video?</p>
                        <p class="answer">0s-1s: A photo of a man and a woman smiling</p>
                        <p class="question">1s-4s: What does the background voice talk about after the scene showing <strong>A photo of a man and a woman smiling</strong>?</p>
                        <p class="answer">1s-4s: Law enforcement is in place</p>
                        <p class="question">1s-4s: What does the video show while the background voice mentions <strong>Law enforcement is in place</strong>?</p>
                        <p class="answer">1s-4s: Three men driving a ship on the sea</p>
                      </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                          <video width="50%" controls>
                              <source src="assets/vid/si.mp4" type="video/mp4">
                              Your browser does not support the video tag.
                          </video>
                          <p><strong>Task: Speaker Identification (SI)</strong></p>
                          <p class="question">Who welcomed someone to Top Notch?</p>
                          <p class="answer">Marie</p>
                      </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                          <video width="50%" controls>
                              <source src="assets/vid/pa.mp4" type="video/mp4">
                              Your browser does not support the video tag.
                          </video>
                          <p><strong>Task: Proactive Alerting (PA)</strong></p>
                          <p class="question">Notify me when the man in a gray top is walking around a garden in the video.</p>
                          <p class="answer">[6s-14s]</p>
                      </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                        <video width="50%" controls>
                            <source src="assets/vid/pt.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p><strong>Task: Proactive Turn-taking (PT)</strong></p>
                        <p class="question">Where did I put the book?</p>
                        <p class="answer">On the bookshelf</p>
                        <p class="question">Where was the ATM located?</p>
                        <p class="answer">Near the window</p>
                        <p class="question">I guess we'll see how it goes, but I'm not too worried about it at the moment.</strong>?</p>
                        <p class="answer">[SILENCE]</p>
                      </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
  </section>
   -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
              <h2 class="title is-3">Data Samples</h2>
              <p style="text-align: center; margin-bottom: 20px;">
                We provide a selection of images in SurgVLM-DB. If you're interested in exploring more, please refer to our complete dataset.
              </p>
              <div>
                  <label for="task-select">Select Task Type:</label>
                  <select id="task-select">
                      <option value="">--Select a Task--</option>
                      <option value="ir">Instrument Recognition</option>
                      <option value="il">Instrument Localization</option>
                      <option value="ilo">Instrument Location</option>
                      <option value="tr">Tissue Recognition</option>
                      <option value="tl">Tissue Localization</option>
                      <option value="pr">Phase Recognition</option>
                      <option value="sr">Step Recognition</option>
                      <option value="ar">Action Recognition</option>
                      <option value="trip">Triplet Recognition</option>
                      <option value="cvs">Critical View of Safety</option>
                  </select>
              </div>
              <div id="results-carousel" class="carousel results-carousel">
                  <div class="box" data-task="ir">
                      <div class="content has-text-centered">
                          <img src="assets/surg_img/instrument recognition.png" alt="Instrument Recognition" style="max-width: 80%; height: auto;">
                          <p><strong>Task: Instrument Recognition</strong></p>
                          <p class="question">Can you name the category for each surgical instrument?</p>
                          <p class="answer">Their categories are Dissecting and grasping forceps.</p>
                      </div>
                  </div>
                  <div class="box" data-task="il">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/instrument localization.png" alt="Instrument Localization" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Instrument Localization</strong></p>
                      <p class="question">Can you locate the Bipolar Forceps in the bottom left area?</p>
                      <p class="answer">The location is [0.179, 0.75, 0.398, 0.999].</p>
                    </div>
                  </div>
                  <div class="box" data-task="ilo">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/instrument location.png" alt="Instrument Location" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Instrument Location</strong></p>
                      <p class="question">Which parts of the image show instruments?</p>
                      <p class="answer">The top right area, middle right area identifies the parts where instruments are visible.</p>
                    </div>
                  </div>
                  <div class="box" data-task="tr">
                    <div class="content has-text-centered">
                        <img src="assets/surg_img/tissue recognition.png" alt="Tissue Recognition" style="max-width: 80%; height: auto;">
                        <p><strong>Task: Tissue Recognition</strong></p>
                        <p class="question">Which tissues are being shown in the surgical picture?</p>
                        <p class="answer">These are the Hepatocystic Triangle Dissection.</p>
                    </div>
                  </div>
                  <div class="box" data-task="tl">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/tissue localizationpng.png" alt="Tissue Localization" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Tissue Localization</strong></p>
                      <p class="question">Mark the Cystic Artery's position in the bottom center area.</p>
                      <p class="answer">The location is [0.357, 0.812, 0.6, 0.969].</p>
                    </div>
                  </div>
                  <div class="box" data-task="pr">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/phase recognition.png" alt="Phase Recognition" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Phase Recognition</strong></p>
                      <p class="question">Based on this image from Cholecystectomy surgery, identify the surgical phase.</p>
                      <p class="answer">The identified phase is gallbladder retraction. In this phase, the gallbladder enclosed in a specimen bag is extracted from the abdominal cavity through a trocar to complete the procedure.</p>
                    </div>
                  </div>
                  <div class="box" data-task="sr">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/step recognition.png" alt="Step Recognition" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Step Recognition</strong></p>
                      <p class="question">What procedural step does this Prostatectomies surgery image illustrate?</p>
                      <p class="answer">The demonstrated step is passing a suture through the bladder neck, and during the step, the surgeon is passing a suture through the bladder neck to aid in urethrovesical anastomosis.</p>
                    </div>
                  </div>
                  <div class="box" data-task="ar">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/action recognition.png" alt="Action Recognition" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Action Recognition</strong></p>
                      <p class="question">What procedure step is the surgeon executing with the needle and suture at this point?</p>
                      <p class="answer">The surgeon is currently pushing the needle through the tissue, advancing it precisely to ensure proper placement.</p>
                    </div>
                  </div>
                  <div class="box" data-task="trip">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/triplet recognition.png" alt="Triplet Recognition" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Triplet Recognition</strong></p>
                      <p class="question">What task is the tool accomplishing with the target in this surgical image?</p>
                      <p class="answer">The tool, a grasper, is performing a retract action on the gallbladder, and the instrument hook is performing a dissect action on the gallbladder in this surgical operation.</p>
                    </div>
                  </div>
                  <div class="box" data-task="cvs">
                    <div class="content has-text-centered">
                      <img src="assets/surg_img/cvs.png" alt="Critical View of Safety" style="max-width: 80%; height: auto;">
                      <p><strong>Task: Critical View of Safety</strong></p>
                      <p class="question">Confirm if each critical view criterion is satisfied: (1) Two tubular structures connected to the gallbladder are visible; (2) The hepatocystic triangle has been cleared of obstruction; (3) The lower gallbladder is dissected to expose the cystic plate.</p>
                      <p class="answer">Criterion 1: No; Criterion 2: No; Criterion 3: Yes.</p>
                    </div>
                  </div>
              </div>
          </div>
      </div>
    </div>
  </section>



  <!-- <section class="section">
      <div class="container is-max-desktop">

      <h2 class="title" id="citation">Citation</h2>

      <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">omnimmi</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts}</span><span class="p">,</span>
      <span class="na">author</span><span class="p">=</span><span class="s">{Zhitao Zeng, Yueming Jin}</span><span class="p">,</span>
      <span class="na">publisher</span><span class="p">=</span><span class="s">{CVPR}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
  <span class="p">}</span> -->

  </code></pre></div>    </div>

    </div>
  </section>


      </div>

      <!-- Footer -->

      <footer class="footer">
      <div class="container">
          <div class="columns is-centered">
              <div class="column is-8">
                  <div class="content">
                      <p>
                          This website is This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                              Commons Attribution-ShareAlike 4.0 International License</a>.
                      </p>
                  </div>
              </div>
          </div>
      </div>
  </footer>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
      integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"
      crossorigin="anonymous"></script>

  <!-- Bulma -->
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/js/bulma-slider.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.24/dist/js/bulma-carousel.min.js"></script>

  <script src="//cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/js/toastr.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js">
  </script>

  <!-- static sort table -->
  <script>
    function sortTable(tableId, n) {
      var table, rows, switching, i, x, y, shouldSwitch, dir, switchcount = 0;
      table = document.getElementById(tableId);
      switching = true;
      dir = "desc"; 
      while (switching) {
        switching = false;
        rows = table.rows;
        for (i = 1; i < (rows.length - 1); i++) {
          shouldSwitch = false;
          x = rows[i].getElementsByTagName("TD")[n];
          y = rows[i + 1].getElementsByTagName("TD")[n];
          
          var xContent = parseFloat(x.innerHTML) || x.innerHTML.toLowerCase();
          var yContent = parseFloat(y.innerHTML) || y.innerHTML.toLowerCase();
  
          if (dir === "asc") {
            if (xContent > yContent) {
              shouldSwitch = true;
              break;
            }
          } else if (dir === "desc") {
            if (xContent < yContent) {
              shouldSwitch = true;
              break;
            }
          }
        }
        if (shouldSwitch) {
          rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);
          switching = true;
          switchcount++;
        } else {
          if (switchcount == 0 && dir === "asc") {
            dir = "desc";
            switching = true;
          }
        }
      }
    }
  
    window.onload = function() {
      sortTable('table1', 16); // Automatically sort the first table by the first column
      sortTable('table2', 16); // Automatically sort the second table by the first column
    };
  </script>
  
  <!-- <script>
    document.getElementById('task-select').addEventListener('change', function() {
        var selectedTask = this.value;
        var boxes = document.querySelectorAll('.box');
  
        boxes.forEach(function(box) {
            if (selectedTask === "" || box.getAttribute('data-task') === selectedTask) {
                box.style.display = 'block';
            } else {
                box.style.display = 'none';
            }
        });
    });
  </script> -->

  <script>
    document.getElementById('task-select').addEventListener('change', function() {
        var selectedTask = this.value;
        var boxes = document.querySelectorAll('.box');

        boxes.forEach(function(box) {
            if (box.getAttribute('data-task') === selectedTask) {
                box.style.display = 'block';
            } else {
                box.style.display = 'none';
            }
        });
    });
  </script>

</body>


</html>
