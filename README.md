# SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence

This repository contains the official website for SurgVLM, a large vision-language model for surgical intelligence.

## Overview

SurgVLM is one of the first surgical foundation models, which is a large vision-language model for surgical intelligence. It is built upon Qwen2.5 and undergoes multimodal projector pretraining to align visual-text representations, followed by instruction tuning to specific surgical tasks.

## Key Features

- **SurgVLM-DB**: A large-scale multimodal surgical dataset comprising over 1.81 million frames with 7.79 million annotations
- **SurgVLM-Bench**: A comprehensive surgical benchmark for vision-language models
- **SurgVLM Models**: Including SurgVLM-7B, SurgVLM-32B, and SurgVLM-72B

## Citation

If you find this work useful for your research, please consider citing:

```bibtex
@misc{surgvlm,
    title={SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence},
    author={Zhitao Zeng, Yueming Jin},
    publisher={CVPR},
    year={2025}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
